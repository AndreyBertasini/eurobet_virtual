{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "parsing data",
   "id": "e4cd217971a9d52e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T10:45:05.556293Z",
     "start_time": "2025-05-15T10:45:00.662363Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import gzip  # Importato per la decompressione manuale gzip\n",
    "import brotli  # Importato per la decompressione manuale Brotli\n",
    "\n",
    "\n",
    "class VirtualSportsCollector:\n",
    "    def __init__(self):\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36',\n",
    "            'Accept': 'application/json, text/plain, */*',\n",
    "            'Accept-Encoding': 'gzip, deflate, br',\n",
    "            'Accept-Language': 'en-US,en;q=0.6',\n",
    "            'Origin': 'https://www.eurobet.it',\n",
    "            'Referer': 'https://www.eurobet.it/',\n",
    "            'X-EB-Accept-Language': 'it_IT',\n",
    "            'X-EB-MarketId': '5',\n",
    "            'X-EB-PlatformId': '1',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Sec-Fetch-Dest': 'empty',\n",
    "            'Sec-Fetch-Mode': 'cors',\n",
    "            'Sec-Fetch-Site': 'same-site'\n",
    "        }\n",
    "        self.base_url = \"https://virtualservice.eurobet.it/virtual-winning-service/virtual-schedule/services/winningresult/68/17/{}\"\n",
    "        self.csv_filename = \"virtual_matches_data.csv\"\n",
    "        self.excel_filename = \"virtual_matches_data.xlsx\"\n",
    "\n",
    "    def create_match_id(self, row):\n",
    "        \"\"\"Crea un identificatore univoco per ogni partita.\"\"\"\n",
    "        date_val = str(row.get('date', ''))\n",
    "        hour_val = str(row.get('hour', ''))\n",
    "        home_team_val = str(row.get('home_team', ''))\n",
    "        away_team_val = str(row.get('away_team', ''))\n",
    "        return f\"{date_val}_{hour_val}_{home_team_val}_{away_team_val}\"\n",
    "\n",
    "    def load_existing_data(self):\n",
    "        \"\"\"Carica i dati esistenti dal CSV, se esiste.\"\"\"\n",
    "        if os.path.exists(self.csv_filename):\n",
    "            try:\n",
    "                dtype_spec = {  # Specifica dtype per colonne potenzialmente problematiche\n",
    "                    'odds_1': 'object', 'result': 'object',\n",
    "                    'over_under_25': 'object', 'odds_over_under_25': 'object',\n",
    "                    'goal_no_goal': 'object', 'odds_goal_no_goal': 'object',\n",
    "                    'home_goals': 'Int64', 'away_goals': 'Int64'  # Usa Int64 per permettere NaN interi\n",
    "                }\n",
    "                df = pd.read_csv(self.csv_filename, dtype=dtype_spec)\n",
    "                if 'datetime' in df.columns:\n",
    "                    df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')\n",
    "                # Rimuovi colonne completamente vuote che potrebbero essere state create da errori precedenti\n",
    "                df.dropna(axis=1, how='all', inplace=True)\n",
    "                return df\n",
    "            except pd.errors.EmptyDataError:\n",
    "                print(f\"Il file CSV {self.csv_filename} è vuoto. Verrà creato un nuovo DataFrame.\")\n",
    "                return pd.DataFrame()\n",
    "            except Exception as e:\n",
    "                print(f\"Errore durante il caricamento del file CSV {self.csv_filename}: {e}\")\n",
    "                return pd.DataFrame()\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    def get_virtual_data(self, start_date, end_date):\n",
    "        \"\"\"Recupera i dati virtuali per l'intervallo di date specificato.\"\"\"\n",
    "        all_matches = []\n",
    "        current_date = start_date\n",
    "\n",
    "        while current_date <= end_date:\n",
    "            date_str = current_date.strftime(\"%d-%m-%Y\")\n",
    "            url = self.base_url.format(date_str)\n",
    "            print(f\"Tentativo di recupero dati per {date_str} da URL: {url}\")\n",
    "            data = None\n",
    "            response = None\n",
    "\n",
    "            try:\n",
    "                response = requests.get(url, headers=self.headers, timeout=25)\n",
    "                response.raise_for_status()\n",
    "\n",
    "                if not response.content:\n",
    "                    print(f\"Risposta vuota ricevuta per {date_str} (Status: {response.status_code}). URL: {url}\")\n",
    "                    time.sleep(1.5)\n",
    "                    current_date += timedelta(days=1)\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    data = response.json()\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Errore di decodifica JSON standard per {date_str}. Status: {response.status_code}\")\n",
    "                    content_encoding = response.headers.get('Content-Encoding', '').lower()\n",
    "                    print(f\"Header Content-Encoding: {content_encoding if content_encoding else 'Non presente'}\")\n",
    "\n",
    "                    decompressed_successfully = False\n",
    "                    if content_encoding == 'br':\n",
    "                        print(\"Tentativo di decompressione Brotli manuale.\")\n",
    "                        try:\n",
    "                            decompressed_content = brotli.decompress(response.content)\n",
    "                            data = json.loads(decompressed_content.decode('utf-8'))\n",
    "                            print(f\"Contenuto Brotli per {date_str} decompresso e parsato manualmente.\")\n",
    "                            decompressed_successfully = True\n",
    "                        except Exception as e_decompress:\n",
    "                            print(f\"Fallimento decompressione/parsing Brotli per {date_str}: {e_decompress}\")\n",
    "\n",
    "                    elif content_encoding == 'gzip' or response.content.startswith(b'\\x1f\\x8b\\x08'):\n",
    "                        print(\"Tentativo di decompressione Gzip manuale.\")\n",
    "                        try:\n",
    "                            decompressed_content = gzip.decompress(response.content)\n",
    "                            data = json.loads(decompressed_content.decode('utf-8'))\n",
    "                            print(f\"Contenuto Gzip per {date_str} decompresso e parsato manualmente.\")\n",
    "                            decompressed_successfully = True\n",
    "                        except Exception as e_decompress:\n",
    "                            print(f\"Fallimento decompressione/parsing Gzip per {date_str}: {e_decompress}\")\n",
    "\n",
    "                    if not decompressed_successfully:\n",
    "                        print(f\"Decodifica JSON fallita per {date_str} anche dopo tentativi manuali (se applicabili).\")\n",
    "                        print(f\"Contenuto grezzo (primi 200 byte): {response.content[:200]}...\")\n",
    "                        time.sleep(1.5)\n",
    "                        current_date += timedelta(days=1)\n",
    "                        continue\n",
    "\n",
    "                # Elaborazione dei dati\n",
    "                if data and 'result' in data and data['result'] is not None and 'groupDate' in data['result']:\n",
    "                    for group in data['result']['groupDate']:\n",
    "                        if 'events' in group and group['events'] is not None:\n",
    "                            for event in group['events']:\n",
    "                                try:\n",
    "                                    # --- Inizio Logica di Parsing Team Names Migliorata ---\n",
    "                                    parsed_home_team = None\n",
    "                                    parsed_away_team = None\n",
    "\n",
    "                                    event_desc_raw = event.get('eventDescription')\n",
    "\n",
    "                                    if isinstance(event_desc_raw, str) and event_desc_raw.strip():\n",
    "                                        cleaned_desc = event_desc_raw.strip()\n",
    "                                        parts = cleaned_desc.split(' - ', 1)  # Divide al massimo una volta\n",
    "\n",
    "                                        if parts[0]:  # Nome squadra casa\n",
    "                                            parsed_home_team = parts[0].strip()\n",
    "                                            if not parsed_home_team:  # Se solo spazi\n",
    "                                                parsed_home_team = None\n",
    "\n",
    "                                        if len(parts) > 1:  # Se il separatore \" - \" è stato trovato\n",
    "                                            if parts[1]:  # Nome squadra ospite\n",
    "                                                parsed_away_team = parts[1].strip()\n",
    "                                                if not parsed_away_team:  # Se solo spazi o stringa vuota\n",
    "                                                    parsed_away_team = None\n",
    "                                            # else: parsed_away_team rimane None (es. \"Team A - \")\n",
    "                                        # else: parsed_away_team rimane None (es. \"Team A\")\n",
    "                                    elif event_desc_raw is not None:  # Se non è stringa ma esiste (improbabile per descrizione)\n",
    "                                        temp_desc = str(event_desc_raw).strip()\n",
    "                                        if temp_desc:\n",
    "                                            parsed_home_team = temp_desc\n",
    "                                    # --- Fine Logica di Parsing Team Names Migliorata ---\n",
    "\n",
    "                                    match_data = {\n",
    "                                        'date': event.get('date'),\n",
    "                                        'hour': event.get('hour'),\n",
    "                                        'home_team': parsed_home_team,  # Usa il nome parsato\n",
    "                                        'away_team': parsed_away_team,  # Usa il nome parsato\n",
    "                                        'score': event.get('finalResult'),\n",
    "                                        'home_goals': None,\n",
    "                                        'away_goals': None,\n",
    "                                        'datetime': None\n",
    "                                    }\n",
    "\n",
    "                                    if event.get('date') and event.get('hour'):\n",
    "                                        try:\n",
    "                                            match_data['datetime'] = pd.to_datetime(f\"{event['date']} {event['hour']}\",\n",
    "                                                                                    format='%d-%m-%Y %H:%M:%S',\n",
    "                                                                                    errors='coerce')\n",
    "                                        except ValueError as ve:\n",
    "                                            print(\n",
    "                                                f\"Errore formato data/ora: {event.get('date')} {event.get('hour')}: {ve}\")\n",
    "\n",
    "                                    if event.get('finalResult') and '-' in event['finalResult']:\n",
    "                                        try:\n",
    "                                            scores = event['finalResult'].split('-')\n",
    "                                            match_data['home_goals'] = int(scores[0])\n",
    "                                            match_data['away_goals'] = int(scores[1])\n",
    "                                        except ValueError:\n",
    "                                            print(\n",
    "                                                f\"Score non parsabile: {event['finalResult']} per {event.get('eventDescription')}\")\n",
    "\n",
    "                                    if 'oddGroup' in event and event['oddGroup'] is not None:\n",
    "                                        for odd_group in event['oddGroup']:\n",
    "                                            bet_abbr = odd_group.get('betDescriptionAbbr')\n",
    "                                            odds_list = odd_group.get('odds')\n",
    "                                            result_desc_list = odd_group.get('resultDescription')\n",
    "\n",
    "                                            if not (odds_list and result_desc_list): continue\n",
    "\n",
    "                                            if bet_abbr == '1X2':\n",
    "                                                match_data['odds_1'] = odds_list[0] if odds_list else None\n",
    "                                                match_data['result'] = result_desc_list[0] if result_desc_list else None\n",
    "                                            elif bet_abbr == 'Under / Over 2.5':\n",
    "                                                match_data['over_under_25'] = result_desc_list[\n",
    "                                                    0] if result_desc_list else None\n",
    "                                                match_data['odds_over_under_25'] = odds_list[0] if odds_list else None\n",
    "                                            elif bet_abbr == 'U/O 2.5':\n",
    "                                                # print(f\"Nota: Trovato 'U/O 2.5' (invece di 'Under / Over 2.5') per {date_str}, evento {event.get('eventDescription')}\")\n",
    "                                                match_data['over_under_25'] = result_desc_list[\n",
    "                                                    0] if result_desc_list else None\n",
    "                                                match_data['odds_over_under_25'] = odds_list[0] if odds_list else None\n",
    "                                            elif bet_abbr == 'Goal/No Goal':\n",
    "                                                match_data['goal_no_goal'] = result_desc_list[\n",
    "                                                    0] if result_desc_list else None\n",
    "                                                match_data['odds_goal_no_goal'] = odds_list[0] if odds_list else None\n",
    "\n",
    "                                    all_matches.append(match_data)\n",
    "                                except Exception as e_inner:\n",
    "                                    print(\n",
    "                                        f\"Errore elaborazione evento ({date_str}): {e_inner}. Evento: {json.dumps(event, indent=2)}\")\n",
    "                        else:\n",
    "                            print(f\"Nessun evento ('events') in groupDate per {date_str}.\")\n",
    "                elif data:\n",
    "                    print(f\"Struttura JSON inattesa per {date_str}. Dati (inizio): {str(data)[:200]}...\")\n",
    "                else:\n",
    "                    print(f\"Nessun dato JSON valido per {date_str} dopo tentativi di decompressione.\")\n",
    "\n",
    "            except requests.exceptions.HTTPError as http_err:\n",
    "                print(f\"Errore HTTP per {date_str}: {http_err} (Status: {response.status_code if response else 'N/A'})\")\n",
    "            except requests.exceptions.ConnectionError as conn_err:\n",
    "                print(f\"Errore di connessione per {date_str}: {conn_err}\")\n",
    "            except requests.exceptions.Timeout as timeout_err:\n",
    "                print(f\"Timeout per {date_str}: {timeout_err}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Errore imprevisto recupero dati ({date_str}): {e}\")\n",
    "\n",
    "            time.sleep(1.5)\n",
    "            current_date += timedelta(days=1)\n",
    "\n",
    "        return pd.DataFrame(all_matches)\n",
    "\n",
    "    def merge_and_save_data(self, new_data):\n",
    "        \"\"\"Unisci i nuovi dati con quelli esistenti, rimuovi sempre i duplicati e salva.\"\"\"\n",
    "        existing_data = self.load_existing_data()\n",
    "\n",
    "        if not existing_data.empty and 'datetime' in existing_data.columns:\n",
    "            existing_data['datetime'] = pd.to_datetime(existing_data['datetime'], errors='coerce')\n",
    "        if not new_data.empty and 'datetime' in new_data.columns:\n",
    "            new_data['datetime'] = pd.to_datetime(new_data['datetime'], errors='coerce')\n",
    "\n",
    "        if new_data.empty:\n",
    "            print(\"Nessun nuovo dato da unire. Processando e risalvando i dati esistenti (se presenti).\")\n",
    "            if existing_data.empty:\n",
    "                print(\"Database esistente è vuoto. Nessun dato da salvare o processare.\")\n",
    "                pd.DataFrame().to_csv(self.csv_filename, index=False)\n",
    "                pd.DataFrame().to_excel(self.excel_filename, index=False)\n",
    "                print(f\"Creati/aggiornati file vuoti: {self.csv_filename} e {self.excel_filename}\")\n",
    "                return pd.DataFrame()\n",
    "            combined_data = existing_data\n",
    "        else:\n",
    "            print(f\"Unendo {len(new_data)} nuove partite con {len(existing_data)} partite esistenti.\")\n",
    "            combined_data = pd.concat([existing_data, new_data], ignore_index=True)\n",
    "\n",
    "        if combined_data.empty:\n",
    "            print(\"Nessun dato (né esistente né nuovo) da processare.\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        print(f\"Dati combinati prima della pulizia: {len(combined_data)} righe.\")\n",
    "\n",
    "        if 'datetime' in combined_data.columns:\n",
    "            initial_rows = len(combined_data)\n",
    "            combined_data.dropna(subset=['datetime'], inplace=True)\n",
    "            if len(combined_data) < initial_rows:\n",
    "                print(f\"Rimosse {initial_rows - len(combined_data)} righe con datetime non valido.\")\n",
    "\n",
    "        combined_data['match_id'] = combined_data.apply(self.create_match_id, axis=1)\n",
    "\n",
    "        combined_data = combined_data[combined_data['match_id'] != \"___\"]\n",
    "        combined_data.dropna(subset=['match_id'], inplace=True)\n",
    "\n",
    "        if combined_data.empty:\n",
    "            print(\"Nessun dato valido rimasto dopo la pulizia iniziale di match_id.\")\n",
    "            pd.DataFrame().to_csv(self.csv_filename, index=False)\n",
    "            pd.DataFrame().to_excel(self.excel_filename, index=False)\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        print(f\"Dati prima della deduplicazione: {len(combined_data)} righe.\")\n",
    "        if 'datetime' in combined_data.columns and not combined_data.empty:\n",
    "            combined_data = combined_data.sort_values('datetime', ascending=False)\n",
    "            combined_data = combined_data.drop_duplicates(subset=['match_id'], keep='first')\n",
    "        elif not combined_data.empty:\n",
    "            combined_data = combined_data.drop_duplicates(subset=['match_id'], keep='first')\n",
    "        print(f\"Dati dopo la deduplicazione: {len(combined_data)} righe.\")\n",
    "\n",
    "        if 'match_id' in combined_data.columns:\n",
    "            combined_data = combined_data.drop('match_id', axis=1)\n",
    "\n",
    "        try:\n",
    "            combined_data.to_csv(self.csv_filename, index=False)\n",
    "            combined_data.to_excel(self.excel_filename, index=False)\n",
    "            print(\n",
    "                f\"Dati salvati con successo ({len(combined_data)} righe) in {self.csv_filename} e {self.excel_filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Errore durante il salvataggio dei file: {e}\")\n",
    "\n",
    "        return combined_data\n",
    "\n",
    "    def collect_data(self, days_to_fetch_count=1):\n",
    "        \"\"\"Metodo principale per raccogliere, elaborare e salvare i dati.\"\"\"\n",
    "        if not isinstance(days_to_fetch_count, int) or days_to_fetch_count < 0:\n",
    "            print(\"Errore: days_to_fetch_count deve essere un intero non negativo (0 per oggi, >0 per giorni passati).\")\n",
    "            return\n",
    "\n",
    "        today = datetime.now()\n",
    "        start_date_of_collection, end_date_of_collection = None, None\n",
    "\n",
    "        if days_to_fetch_count == 0:\n",
    "            start_date_of_collection = today\n",
    "            end_date_of_collection = today\n",
    "            print(f\"Richiesta dati per oggi: {today.strftime('%d-%m-%Y')}\")\n",
    "        else:\n",
    "            end_date_of_collection = today - timedelta(days=1)\n",
    "            start_date_of_collection = end_date_of_collection - timedelta(days=days_to_fetch_count - 1)\n",
    "            if days_to_fetch_count == 1:\n",
    "                print(f\"Richiesta dati per ieri: {end_date_of_collection.strftime('%d-%m-%Y')}\")\n",
    "            else:\n",
    "                print(\n",
    "                    f\"Richiesta dati dal {start_date_of_collection.strftime('%d-%m-%Y')} al {end_date_of_collection.strftime('%d-%m-%Y')}\")\n",
    "\n",
    "        new_data = pd.DataFrame()\n",
    "        if start_date_of_collection <= end_date_of_collection:\n",
    "            new_data = self.get_virtual_data(start_date_of_collection, end_date_of_collection)\n",
    "        else:\n",
    "            print(\n",
    "                f\"Intervallo date non valido: start {start_date_of_collection}, end {end_date_of_collection}. Salto recupero dati.\")\n",
    "\n",
    "        if not new_data.empty:\n",
    "            print(f\"Recuperate {len(new_data)} nuove partite dalla fonte.\")\n",
    "        else:\n",
    "            print(\"Nessun nuovo dato raccolto dalla fonte.\")\n",
    "\n",
    "        final_data = self.merge_and_save_data(new_data)\n",
    "\n",
    "        if final_data is not None and not final_data.empty:\n",
    "            print(f\"Database finale contiene {len(final_data)} partite.\")\n",
    "        elif final_data is not None and final_data.empty:\n",
    "            print(\"Il database finale è vuoto.\")\n",
    "        else:\n",
    "            print(\"Errore: final_data è None dopo merge_and_save_data.\")\n",
    "\n",
    "\n",
    "def main(days_to_fetch_count=1):\n",
    "    \"\"\"Funzione principale per avviare il collettore di dati.\"\"\"\n",
    "    collector = VirtualSportsCollector()\n",
    "    collector.collect_data(days_to_fetch_count)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    GIORNI_DA_RECUPERARE = 0\n",
    "\n",
    "    print(f\"Avvio script. GIORNI_DA_RECUPERARE impostato a: {GIORNI_DA_RECUPERARE}\")\n",
    "    main(days_to_fetch_count=GIORNI_DA_RECUPERARE)\n",
    "    print(\"Script terminato.\")\n"
   ],
   "id": "c2230109c9cc5093",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avvio script. GIORNI_DA_RECUPERARE impostato a: 0\n",
      "Richiesta dati per oggi: 15-05-2025\n",
      "Tentativo di recupero dati per 15-05-2025 da URL: https://virtualservice.eurobet.it/virtual-winning-service/virtual-schedule/services/winningresult/68/17/15-05-2025\n",
      "Recuperate 28 nuove partite dalla fonte.\n",
      "Unendo 28 nuove partite con 30891 partite esistenti.\n",
      "Dati combinati prima della pulizia: 30919 righe.\n",
      "Dati prima della deduplicazione: 30919 righe.\n",
      "Dati dopo la deduplicazione: 30891 righe.\n",
      "Dati salvati con successo (30891 righe) in virtual_matches_data.csv e virtual_matches_data.xlsx\n",
      "Database finale contiene 30891 partite.\n",
      "Script terminato.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "lettura df",
   "id": "f53ec963ae9bb0f"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "data = pd.read_csv('virtual_matches_data.csv')",
   "id": "e612fe1eb70941fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "filtro per model data",
   "id": "faa40aa0d21234cc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T10:44:59.213981Z",
     "start_time": "2025-05-15T10:44:59.207780Z"
    }
   },
   "cell_type": "code",
   "source": "data = data[['date', 'hour', 'home_team', 'away_team', 'home_goals', 'away_goals', 'result']]",
   "id": "3090e15a5b8d9d6e",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T10:44:59.276555Z",
     "start_time": "2025-05-15T10:44:59.236691Z"
    }
   },
   "cell_type": "code",
   "source": "data.to_csv('model_data.csv',index = False)",
   "id": "bdae47d1d7a3e43b",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T10:44:59.304416Z",
     "start_time": "2025-05-15T10:44:59.294021Z"
    }
   },
   "cell_type": "code",
   "source": "data.head(10)",
   "id": "b8281a877ffa7ea0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         date      hour  home_team    away_team  home_goals  away_goals result\n",
       "0  15-05-2025  12:40:00    Messico       Serbia           2           2      X\n",
       "1  15-05-2025  12:34:00     Italia       Spagna           0           1      2\n",
       "2  15-05-2025  12:28:00    Croazia       Olanda           3           1      1\n",
       "3  15-05-2025  12:22:00   Colombia      Messico           1           2      2\n",
       "4  15-05-2025  12:16:00       Cile  Inghilterra           0           2      2\n",
       "5  15-05-2025  12:10:00   Germania      Nigeria           0           1      2\n",
       "6  15-05-2025  12:04:00    Francia  Inghilterra           0           0      X\n",
       "7  15-05-2025  11:58:00    Brasile       Spagna           2           0      1\n",
       "8  15-05-2025  11:52:00     Belgio     Svizzera           1           0      1\n",
       "9  15-05-2025  11:46:00  Argentina      Turchia           0           3      2"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>hour</th>\n",
       "      <th>home_team</th>\n",
       "      <th>away_team</th>\n",
       "      <th>home_goals</th>\n",
       "      <th>away_goals</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15-05-2025</td>\n",
       "      <td>12:40:00</td>\n",
       "      <td>Messico</td>\n",
       "      <td>Serbia</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15-05-2025</td>\n",
       "      <td>12:34:00</td>\n",
       "      <td>Italia</td>\n",
       "      <td>Spagna</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15-05-2025</td>\n",
       "      <td>12:28:00</td>\n",
       "      <td>Croazia</td>\n",
       "      <td>Olanda</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15-05-2025</td>\n",
       "      <td>12:22:00</td>\n",
       "      <td>Colombia</td>\n",
       "      <td>Messico</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15-05-2025</td>\n",
       "      <td>12:16:00</td>\n",
       "      <td>Cile</td>\n",
       "      <td>Inghilterra</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>15-05-2025</td>\n",
       "      <td>12:10:00</td>\n",
       "      <td>Germania</td>\n",
       "      <td>Nigeria</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>15-05-2025</td>\n",
       "      <td>12:04:00</td>\n",
       "      <td>Francia</td>\n",
       "      <td>Inghilterra</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>15-05-2025</td>\n",
       "      <td>11:58:00</td>\n",
       "      <td>Brasile</td>\n",
       "      <td>Spagna</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>15-05-2025</td>\n",
       "      <td>11:52:00</td>\n",
       "      <td>Belgio</td>\n",
       "      <td>Svizzera</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15-05-2025</td>\n",
       "      <td>11:46:00</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Turchia</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Predizione Catboost",
   "id": "4c1788b8bc922002"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T10:45:00.623812Z",
     "start_time": "2025-05-15T10:44:59.355713Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Impostazioni per una migliore visualizzazione\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format) # Formattazione per i float\n",
    "\n",
    "# Funzione helper per calcolare la lunghezza della striscia corrente di 'non-eventi'\n",
    "# Un 'evento' (condizione negativa) resetta la striscia.\n",
    "# Modificata per essere più generica nel nome, ma la logica è la stessa.\n",
    "def _calculate_streak_ending_here(series_is_event_resets_streak):\n",
    "    \"\"\"\n",
    "    Calcola la lunghezza della striscia corrente di 'non-eventi'.\n",
    "    Un 'evento' (series_is_event_resets_streak == True) resetta la striscia a 0.\n",
    "    Altrimenti, la striscia incrementa.\n",
    "    \"\"\"\n",
    "    streaks = []\n",
    "    current_streak = 0\n",
    "    for is_reset_event in series_is_event_resets_streak:\n",
    "        if is_reset_event: # Se l'evento che resetta la striscia si verifica\n",
    "            current_streak = 0\n",
    "        else: # L'evento non si verifica, quindi la striscia continua (o inizia se era 0)\n",
    "            current_streak += 1\n",
    "        streaks.append(current_streak)\n",
    "    return pd.Series(streaks, index=series_is_event_resets_streak.index)\n",
    "\n",
    "print(\"Ciao Andrea! Iniziamo l'analisi per la predizione della distanza tra pareggi usando CatBoost.\")\n",
    "\n",
    "# Caricamento dei dati\n",
    "try:\n",
    "    data = pd.read_csv('model_data.csv')\n",
    "    print(f\"Dati caricati con successo. Numero di righe: {len(data)}, Numero di colonne: {len(data.columns)}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Errore: File 'model_data.csv' non trovato. Assicurati che sia nel percorso corretto.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Errore durante il caricamento dei dati: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- 1. Preparazione Dati Iniziale ---\n",
    "print(\"\\n--- Inizio Fase 1: Preparazione Dati Iniziale ---\")\n",
    "\n",
    "df = data.copy()\n",
    "\n",
    "# Conversione 'date' e 'hour' in datetime\n",
    "# Gestione del formato data: il file CSV originale ha i dati più recenti in alto.\n",
    "# Ordiniamo esplicitamente per data/ora ascendente.\n",
    "try:\n",
    "    df['datetime'] = pd.to_datetime(df['date'] + ' ' + df['hour'], format='%d/%m/%Y %H:%M', errors='coerce')\n",
    "except ValueError: # Fallback per altri formati se necessario, o se il formato non è consistente\n",
    "    print(\"Formato data/ora non riconosciuto come 'dd/mm/yyyy HH:MM'. Tentativo con infer_datetime_format.\")\n",
    "    try:\n",
    "        df['datetime'] = pd.to_datetime(df['date'] + ' ' + df['hour'], infer_datetime_format=True, errors='coerce')\n",
    "    except Exception as e_dt:\n",
    "        print(f\"Errore finale nella conversione datetime: {e_dt}\")\n",
    "        df['datetime'] = pd.NaT # Imposta a Not a Time in caso di fallimento\n",
    "\n",
    "if df['datetime'].isnull().any():\n",
    "    print(\"Attenzione: Alcune date non sono state convertite correttamente in datetime.\")\n",
    "    print(df[df['datetime'].isnull()][['date', 'hour']].head())\n",
    "    # Potresti voler gestire queste righe o uscire, a seconda della gravità\n",
    "    # exit()\n",
    "\n",
    "df.dropna(subset=['datetime'], inplace=True)\n",
    "if df.empty:\n",
    "    print(\"DataFrame vuoto dopo la rimozione di righe con datetime non validi.\")\n",
    "    exit()\n",
    "\n",
    "# Ordinamento cronologico: ESSENZIALE per calcoli di strisce e split temporali\n",
    "df = df.sort_values('datetime').reset_index(drop=True)\n",
    "df['match_id'] = df.index # Utile per riferimenti futuri se necessario\n",
    "\n",
    "# Definizione di base del pareggio\n",
    "df['is_draw'] = (df['home_goals'] == df['away_goals']).astype(int)\n",
    "\n",
    "# Feature: Global distance since last draw (NON-LEAKY)\n",
    "# Questa feature calcola, per ogni partita, quanti incontri sono trascorsi dall'ultimo pareggio globale.\n",
    "# Se la partita corrente è un pareggio, questa feature indica la lunghezza della striscia di non-pareggi\n",
    "# che si è appena conclusa.\n",
    "# Input per _calculate_streak_ending_here è df['is_draw'].\n",
    "# Se is_draw è True (pareggio), la striscia di non-pareggi si interrompe (reset a 0).\n",
    "# Se is_draw è False (non pareggio), la striscia di non-pareggi continua.\n",
    "df['_global_current_non_draw_streak'] = _calculate_streak_ending_here(df['is_draw'])\n",
    "# _global_current_non_draw_streak:\n",
    "# Esempio: is_draw = [F, F, F, T, F, T] -> [0,0,0,1,0,1]\n",
    "# _calc_streak :       [1, 2, 3, 0, 1, 0] (conta i non-pareggi, resetta a 0 sul pareggio)\n",
    "\n",
    "# global_distance_since_last_draw è il valore di _global_current_non_draw_streak della partita precedente.\n",
    "# Rappresenta il numero di non-pareggi consecutivi *prima* della partita corrente.\n",
    "df['global_distance_since_last_draw'] = df['_global_current_non_draw_streak'].shift(1)\n",
    "df['global_distance_since_last_draw'].fillna(0, inplace=True) # Per la prima partita, o se la precedente era un pareggio.\n",
    "df.drop(columns=['_global_current_non_draw_streak'], inplace=True)\n",
    "\n",
    "print(\"Feature 'global_distance_since_last_draw' calcolata.\")\n",
    "print(df[['datetime', 'is_draw', 'global_distance_since_last_draw']].head())\n",
    "print(\"\\n--- Fine Fase 1: Preparazione Dati Iniziale ---\")\n",
    "\n",
    "\n",
    "# --- 2. Estrazione e Preparazione della Sequenza di Intervalli tra Pareggi ---\n",
    "print(\"\\n--- Inizio Fase 2: Estrazione Sequenza Intervalli tra Pareggi ---\")\n",
    "\n",
    "# Filtriamo le partite che sono finite in pareggio\n",
    "df_draw_events = df[df['is_draw'] == 1].copy()\n",
    "\n",
    "# La colonna 'global_distance_since_last_draw' in df_draw_events ora rappresenta\n",
    "# la lunghezza della striscia di non-pareggi che si è conclusa con quel pareggio.\n",
    "# Questo è il nostro \"intervallo tra pareggi\".\n",
    "df_draw_events.rename(columns={'global_distance_since_last_draw': 'draw_interval_length'}, inplace=True)\n",
    "\n",
    "# Potremmo voler escludere intervalli di lunghezza 0 se il primo match del dataset è un pareggio,\n",
    "# o se ci sono pareggi consecutivi (improbabile con questa definizione di 'global_distance_since_last_draw'\n",
    "# che dovrebbe dare 0 per un pareggio immediatamente successivo a un altro).\n",
    "# Per ora, li manteniamo, ma è un punto da considerare.\n",
    "# df_draw_events = df_draw_events[df_draw_events['draw_interval_length'] > 0]\n",
    "\n",
    "if df_draw_events.empty:\n",
    "    print(\"Nessun evento di pareggio trovato nel dataset. Impossibile procedere.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Numero di eventi di pareggio identificati: {len(df_draw_events)}\")\n",
    "print(\"Primi eventi di pareggio e la lunghezza dell'intervallo corrispondente:\")\n",
    "print(df_draw_events[['datetime', 'draw_interval_length']].head())\n",
    "\n",
    "# Creazione di features laggate per predire il prossimo intervallo\n",
    "N_LAGS = 5 # Numero di intervalli passati da usare come features\n",
    "print(f\"\\nCreazione di {N_LAGS} features laggate dagli intervalli tra pareggi...\")\n",
    "\n",
    "for i in range(1, N_LAGS + 1):\n",
    "    df_draw_events[f'lag_{i}_interval'] = df_draw_events['draw_interval_length'].shift(i)\n",
    "\n",
    "# Aggiunta di altre features basate sulla sequenza storica, se desiderato:\n",
    "# Esempio: media mobile degli ultimi N_LAGS intervalli (escludendo il corrente)\n",
    "# df_draw_events['moving_avg_interval'] = df_draw_events['draw_interval_length'].shift(1).rolling(window=N_LAGS, min_periods=1).mean()\n",
    "\n",
    "# Rimuoviamo le righe con NaN generati dalla creazione dei lag (le prime N_LAGS righe)\n",
    "df_draw_events.dropna(inplace=True)\n",
    "\n",
    "if df_draw_events.empty:\n",
    "    print(f\"DataFrame degli eventi di pareggio vuoto dopo aver creato {N_LAGS} lag e rimosso i NaN.\")\n",
    "    print(\"Potrebbe essere necessario un dataset più grande o un numero inferiore di lag.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Dimensioni del dataset per il modello dopo la creazione dei lag: {df_draw_events.shape}\")\n",
    "print(\"Esempio di dati con features laggate:\")\n",
    "cols_to_show_lags = ['datetime', 'draw_interval_length'] + [f'lag_{i}_interval' for i in range(1, N_LAGS + 1)]\n",
    "print(df_draw_events[cols_to_show_lags].head())\n",
    "\n",
    "print(\"\\n--- Fine Fase 2: Estrazione Sequenza Intervalli tra Pareggi ---\")\n",
    "\n",
    "\n",
    "# --- 3. Preparazione Dati per il Modello CatBoost ---\n",
    "print(\"\\n--- Inizio Fase 3: Preparazione Dati per CatBoost ---\")\n",
    "features_for_model = [f'lag_{i}_interval' for i in range(1, N_LAGS + 1)]\n",
    "# if 'moving_avg_interval' in df_draw_events.columns:\n",
    "#     features_for_model.append('moving_avg_interval')\n",
    "\n",
    "target_variable = 'draw_interval_length'\n",
    "\n",
    "X = df_draw_events[features_for_model]\n",
    "y = df_draw_events[target_variable]\n",
    "\n",
    "if len(X) < 50: # Valore arbitrario, ma un dataset molto piccolo è problematico\n",
    "    print(f\"Attenzione: dataset per il modello molto piccolo ({len(X)} campioni). Le performance potrebbero essere scarse.\")\n",
    "\n",
    "# Split temporale dei dati (essendo una serie storica)\n",
    "# Manteniamo l'ordine cronologico\n",
    "train_size_ratio, val_size_ratio = 0.7, 0.15 # 70% train, 15% validation, 15% test\n",
    "\n",
    "train_idx = int(train_size_ratio * len(X))\n",
    "val_idx = int((train_size_ratio + val_size_ratio) * len(X))\n",
    "\n",
    "X_train, y_train = X.iloc[:train_idx], y.iloc[:train_idx]\n",
    "X_val, y_val = X.iloc[train_idx:val_idx], y.iloc[train_idx:val_idx]\n",
    "X_test, y_test = X.iloc[val_idx:], y.iloc[val_idx:]\n",
    "\n",
    "print(f\"Dimensioni dei set: Train={X_train.shape}, Validation={X_val.shape}, Test={X_test.shape}\")\n",
    "\n",
    "if X_train.empty or X_test.empty:\n",
    "    print(\"Errore: Train o Test set è vuoto. Controllare la dimensione del dataset e le percentuali di split.\")\n",
    "    exit()\n",
    "# Il validation set può essere vuoto se val_size_ratio è 0 o troppo piccolo.\n",
    "\n",
    "print(\"\\n--- Fine Fase 3: Preparazione Dati per CatBoost ---\")\n",
    "\n",
    "\n",
    "# --- 4. Sviluppo del Modello CatBoost Regressor ---\n",
    "print(\"\\n--- Inizio Fase 4: Sviluppo del Modello CatBoost ---\")\n",
    "\n",
    "model = CatBoostRegressor(\n",
    "    iterations=1000,        # Numero di alberi\n",
    "    learning_rate=0.05,\n",
    "    depth=6,\n",
    "    loss_function='RMSE',   # Metrica di loss comune per la regressione\n",
    "    eval_metric='MAE',      # Metrica per l'early stopping e la valutazione intermedia\n",
    "    random_seed=42,\n",
    "    verbose=200,            # Stampa info ogni 200 iterazioni\n",
    "    # early_stopping_rounds=50 # Attiva l'early stopping\n",
    ")\n",
    "\n",
    "print(\"Inizio addestramento del modello CatBoostRegressor...\")\n",
    "fit_params = {}\n",
    "if not X_val.empty and not y_val.empty:\n",
    "    fit_params['eval_set'] = (X_val, y_val)\n",
    "    fit_params['early_stopping_rounds'] = 50\n",
    "    print(\"Utilizzo di un set di validazione per l'early stopping.\")\n",
    "else:\n",
    "    print(\"Nessun set di validazione fornito o è vuoto. Addestramento senza early stopping basato su eval_set.\")\n",
    "\n",
    "try:\n",
    "    model.fit(X_train, y_train, **fit_params)\n",
    "    print(\"Modello CatBoost addestrato con successo.\")\n",
    "    model_fitted_successfully = True\n",
    "except Exception as e:\n",
    "    print(f\"Errore durante l'addestramento del modello CatBoost: {e}\")\n",
    "    model_fitted_successfully = False\n",
    "\n",
    "print(\"\\n--- Fine Fase 4: Sviluppo del Modello CatBoost ---\")\n",
    "\n",
    "\n",
    "# --- 5. Valutazione del Modello ---\n",
    "if model_fitted_successfully and not X_test.empty:\n",
    "    print(\"\\n--- Inizio Fase 5: Valutazione del Modello ---\")\n",
    "    y_pred_test = model.predict(X_test)\n",
    "\n",
    "    mse = mean_squared_error(y_test, y_pred_test)\n",
    "    mae = mean_absolute_error(y_test, y_pred_test)\n",
    "    r2 = r2_score(y_test, y_pred_test)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    print(\"\\nValutazione sul Test Set:\")\n",
    "    print(f\"  Mean Squared Error (MSE): {mse:.4f}\")\n",
    "    print(f\"  Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "    print(f\"  Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "    print(f\"  R-squared (R²): {r2:.4f}\")\n",
    "\n",
    "    # Plot Actual vs. Predicted\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test, y_pred_test, alpha=0.6, edgecolors='w', linewidth=0.5)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2) # Linea y=x\n",
    "    plt.xlabel(\"Valori Reali (Lunghezza Intervallo)\")\n",
    "    plt.ylabel(\"Valori Predetti (Lunghezza Intervallo)\")\n",
    "    plt.title(\"Confronto Valori Reali vs. Predetti sul Test Set\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Feature Importance\n",
    "    if hasattr(model, 'get_feature_importance'):\n",
    "        feature_importances = model.get_feature_importance()\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': features_for_model,\n",
    "            'importance': feature_importances\n",
    "        }).sort_values('importance', ascending=False)\n",
    "\n",
    "        plt.figure(figsize=(10, max(6, len(features_for_model) * 0.5)))\n",
    "        sns.barplot(x='importance', y='feature', data=importance_df)\n",
    "        plt.title('Importanza delle Features (CatBoost)')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        print(\"\\nImportanza delle features:\\n\", importance_df)\n",
    "\n",
    "    print(\"\\n--- Fine Fase 5: Valutazione del Modello ---\")\n",
    "elif not model_fitted_successfully:\n",
    "    print(\"\\n--- Fase 5: Valutazione del Modello SKIPPATA (addestramento fallito) ---\")\n",
    "else:\n",
    "    print(\"\\n--- Fase 5: Valutazione del Modello SKIPPATA (Test set vuoto) ---\")\n",
    "\n",
    "\n",
    "# --- 6. Predizione del Prossimo Intervallo (Esempio) ---\n",
    "if model_fitted_successfully and len(y) > N_LAGS: # Assicurati di avere abbastanza dati storici\n",
    "    print(\"\\n--- Inizio Fase 6: Predizione del Prossimo Intervallo ---\")\n",
    "\n",
    "    # Prendiamo gli ultimi N_LAGS intervalli osservati dall'intero dataset df_draw_events\n",
    "    # Questi sarebbero gli input per predire il successivo intervallo non ancora osservato.\n",
    "    last_known_intervals_series = df_draw_events['draw_interval_length'].tail(N_LAGS)\n",
    "\n",
    "    if len(last_known_intervals_series) == N_LAGS:\n",
    "        # Prepara l'input per il modello. CatBoost si aspetta un DataFrame\n",
    "        # con nomi di colonna corrispondenti a quelli usati durante l'addestramento.\n",
    "        # I valori devono essere nell'ordine corretto: lag_1 è il più recente dei lag, lag_N è il più vecchio.\n",
    "        # Quindi, se last_known_intervals_series è [I(t-N+1), ..., I(t-1), I(t)],\n",
    "        # lag_1 = I(t), lag_2 = I(t-1), ..., lag_N = I(t-N+1)\n",
    "\n",
    "        input_data_for_prediction = {}\n",
    "        for i in range(N_LAGS):\n",
    "            # lag_{i+1}_interval corrisponde a last_known_intervals_series.iloc[N_LAGS - 1 - i]\n",
    "            input_data_for_prediction[f'lag_{i+1}_interval'] = [last_known_intervals_series.iloc[N_LAGS - 1 - i]]\n",
    "\n",
    "        input_df_for_prediction = pd.DataFrame(input_data_for_prediction, columns=features_for_model)\n",
    "\n",
    "        print(\"\\nUltimi intervalli osservati (usati come input per la predizione):\")\n",
    "        print(last_known_intervals_series)\n",
    "        print(\"\\nDataFrame di input per la predizione del prossimo intervallo:\")\n",
    "        print(input_df_for_prediction)\n",
    "\n",
    "        predicted_next_interval = model.predict(input_df_for_prediction)\n",
    "        print(f\"\\nPredizione per la lunghezza del prossimo intervallo tra pareggi: {predicted_next_interval[0]:.2f} partite\")\n",
    "    else:\n",
    "        print(\"Non ci sono abbastanza dati storici recenti per fare una predizione del prossimo intervallo.\")\n",
    "\n",
    "    print(\"\\n--- Fine Fase 6: Predizione del Prossimo Intervallo ---\")\n",
    "else:\n",
    "    print(\"\\n--- Fase 6: Predizione del Prossimo Intervallo SKIPPATA ---\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Fine dell'analisi. Ciao Andrea! ---\")\n",
    "\n"
   ],
   "id": "20537e841aa8937",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ciao Andrea! Iniziamo l'analisi per la predizione della distanza tra pareggi usando CatBoost.\n",
      "Dati caricati con successo. Numero di righe: 30891, Numero di colonne: 7\n",
      "\n",
      "--- Inizio Fase 1: Preparazione Dati Iniziale ---\n",
      "Attenzione: Alcune date non sono state convertite correttamente in datetime.\n",
      "         date      hour\n",
      "0  15-05-2025  12:40:00\n",
      "1  15-05-2025  12:34:00\n",
      "2  15-05-2025  12:28:00\n",
      "3  15-05-2025  12:22:00\n",
      "4  15-05-2025  12:16:00\n",
      "DataFrame vuoto dopo la rimozione di righe con datetime non validi.\n",
      "Feature 'global_distance_since_last_draw' calcolata.\n",
      "Empty DataFrame\n",
      "Columns: [datetime, is_draw, global_distance_since_last_draw]\n",
      "Index: []\n",
      "\n",
      "--- Fine Fase 1: Preparazione Dati Iniziale ---\n",
      "\n",
      "--- Inizio Fase 2: Estrazione Sequenza Intervalli tra Pareggi ---\n",
      "Nessun evento di pareggio trovato nel dataset. Impossibile procedere.\n",
      "Numero di eventi di pareggio identificati: 0\n",
      "Primi eventi di pareggio e la lunghezza dell'intervallo corrispondente:\n",
      "Empty DataFrame\n",
      "Columns: [datetime, draw_interval_length]\n",
      "Index: []\n",
      "\n",
      "Creazione di 5 features laggate dagli intervalli tra pareggi...\n",
      "DataFrame degli eventi di pareggio vuoto dopo aver creato 5 lag e rimosso i NaN.\n",
      "Potrebbe essere necessario un dataset più grande o un numero inferiore di lag.\n",
      "Dimensioni del dataset per il modello dopo la creazione dei lag: (0, 16)\n",
      "Esempio di dati con features laggate:\n",
      "Empty DataFrame\n",
      "Columns: [datetime, draw_interval_length, lag_1_interval, lag_2_interval, lag_3_interval, lag_4_interval, lag_5_interval]\n",
      "Index: []\n",
      "\n",
      "--- Fine Fase 2: Estrazione Sequenza Intervalli tra Pareggi ---\n",
      "\n",
      "--- Inizio Fase 3: Preparazione Dati per CatBoost ---\n",
      "Attenzione: dataset per il modello molto piccolo (0 campioni). Le performance potrebbero essere scarse.\n",
      "Dimensioni dei set: Train=(0, 5), Validation=(0, 5), Test=(0, 5)\n",
      "Errore: Train o Test set è vuoto. Controllare la dimensione del dataset e le percentuali di split.\n",
      "\n",
      "--- Fine Fase 3: Preparazione Dati per CatBoost ---\n",
      "\n",
      "--- Inizio Fase 4: Sviluppo del Modello CatBoost ---\n",
      "Inizio addestramento del modello CatBoostRegressor...\n",
      "Nessun set di validazione fornito o è vuoto. Addestramento senza early stopping basato su eval_set.\n",
      "Errore durante l'addestramento del modello CatBoost: Labels variable is empty.\n",
      "\n",
      "--- Fine Fase 4: Sviluppo del Modello CatBoost ---\n",
      "\n",
      "--- Fase 5: Valutazione del Modello SKIPPATA (addestramento fallito) ---\n",
      "\n",
      "--- Fase 6: Predizione del Prossimo Intervallo SKIPPATA ---\n",
      "\n",
      "--- Fine dell'analisi. Ciao Andrea! ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dyshkantiuk_andrii\\AppData\\Local\\Temp\\ipykernel_23252\\4204359697.py:97: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['global_distance_since_last_draw'].fillna(0, inplace=True) # Per la prima partita, o se la precedente era un pareggio.\n"
     ]
    }
   ],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
