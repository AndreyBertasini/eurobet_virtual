{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "preleviamo dati da goldbet big match",
   "id": "df82b2140d668b51"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-22T08:54:17.792020Z",
     "start_time": "2024-11-22T08:54:10.028635Z"
    }
   },
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "\n",
    "class VirtualSportsCollector:\n",
    "    def __init__(self):\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36',\n",
    "            'Accept': 'application/json, text/plain, */*',\n",
    "            'Accept-Encoding': 'gzip, deflate, br, zstd',\n",
    "            'Accept-Language': 'en-US,en;q=0.6',\n",
    "            'Origin': 'https://www.eurobet.it',\n",
    "            'X-EB-Accept-Language': 'it_IT',\n",
    "            'X-EB-MarketId': '5',\n",
    "            'X-EB-PlatformId': '1',\n",
    "            'Connection': 'keep-alive'\n",
    "        }\n",
    "        self.base_url = \"https://virtualservice.eurobet.it/virtual-winning-service/virtual-schedule/services/winningresult/55/22/{}\"\n",
    "        self.csv_filename = \"virtual_matches_data.csv\"\n",
    "        self.excel_filename = \"virtual_matches_data.xlsx\"\n",
    "\n",
    "    def create_match_id(self, row):\n",
    "        \"\"\"Create a unique identifier for each match\"\"\"\n",
    "        return f\"{row['date']}_{row['hour']}_{row['home_team']}_{row['away_team']}\"\n",
    "\n",
    "    def load_existing_data(self):\n",
    "        \"\"\"Load existing data from CSV if it exists\"\"\"\n",
    "        if os.path.exists(self.csv_filename):\n",
    "            return pd.read_csv(self.csv_filename)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    def get_virtual_data(self, start_date, end_date):\n",
    "        all_matches = []\n",
    "        current_date = start_date\n",
    "\n",
    "        while current_date <= end_date:\n",
    "            date_str = current_date.strftime(\"%d-%m-%Y\")\n",
    "            url = self.base_url.format(date_str)\n",
    "\n",
    "            try:\n",
    "                response = requests.get(url, headers=self.headers)\n",
    "                if response.status_code == 200:\n",
    "                    data = response.json()\n",
    "                    if 'result' in data and 'groupDate' in data['result']:\n",
    "                        for group in data['result']['groupDate']:\n",
    "                            for event in group['events']:\n",
    "                                match_data = {\n",
    "                                    'date': event['date'],\n",
    "                                    'hour': event['hour'],\n",
    "                                    'home_team': event['eventDescription'].split(' - ')[0],\n",
    "                                    'away_team': event['eventDescription'].split(' - ')[1],\n",
    "                                    'score': event['finalResult'],\n",
    "                                    'home_goals': int(event['finalResult'].split('-')[0]),\n",
    "                                    'away_goals': int(event['finalResult'].split('-')[1]),\n",
    "                                    'datetime': pd.to_datetime(f\"{event['date']} {event['hour']}\", format='%d-%m-%Y %H:%M:%S')\n",
    "                                }\n",
    "\n",
    "                                for odd_group in event['oddGroup']:\n",
    "                                    if odd_group['betDescriptionAbbr'] == '1X2':\n",
    "                                        match_data['odds_1'] = odd_group['odds'][0]\n",
    "                                        match_data['result'] = odd_group['resultDescription'][0]\n",
    "                                    elif odd_group['betDescriptionAbbr'] == 'U/O 2.5':\n",
    "                                        match_data['over_under_25'] = odd_group['resultDescription'][0]\n",
    "                                        match_data['odds_over_under_25'] = odd_group['odds'][0]\n",
    "                                    elif odd_group['betDescriptionAbbr'] == 'Goal/No Goal':\n",
    "                                        match_data['goal_no_goal'] = odd_group['resultDescription'][0]\n",
    "                                        match_data['odds_goal_no_goal'] = odd_group['odds'][0]\n",
    "\n",
    "                                all_matches.append(match_data)\n",
    "\n",
    "                time.sleep(1)  # Respect rate limiting\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching data for {date_str}: {e}\")\n",
    "\n",
    "            current_date += timedelta(days=1)\n",
    "\n",
    "        return pd.DataFrame(all_matches)\n",
    "\n",
    "    def merge_and_save_data(self, new_data):\n",
    "        \"\"\"Merge new data with existing data, remove duplicates, and save\"\"\"\n",
    "        existing_data = self.load_existing_data()\n",
    "\n",
    "        if not existing_data.empty:\n",
    "            # Convert datetime column in existing data if it's not already datetime\n",
    "            existing_data['datetime'] = pd.to_datetime(existing_data['datetime'])\n",
    "\n",
    "        # Combine existing and new data\n",
    "        combined_data = pd.concat([existing_data, new_data], ignore_index=True)\n",
    "\n",
    "        # Create unique identifier for each match\n",
    "        combined_data['match_id'] = combined_data.apply(self.create_match_id, axis=1)\n",
    "\n",
    "        # Remove duplicates based on match_id\n",
    "        combined_data = combined_data.drop_duplicates(subset=['match_id'], keep='first')\n",
    "\n",
    "        # Sort by datetime in descending order (most recent first)\n",
    "        combined_data = combined_data.sort_values('datetime', ascending=False)\n",
    "\n",
    "        # Drop the match_id column as it's no longer needed\n",
    "        combined_data = combined_data.drop('match_id', axis=1)\n",
    "\n",
    "        # Save to CSV and Excel\n",
    "        combined_data.to_csv(self.csv_filename, index=False)\n",
    "        combined_data.to_excel(self.excel_filename, index=False)\n",
    "\n",
    "        return combined_data\n",
    "\n",
    "    def collect_data(self, days_back=1):\n",
    "        \"\"\"Main method to collect and process data\"\"\"\n",
    "        end_date = datetime.now()\n",
    "        start_date = end_date - timedelta(days=days_back)\n",
    "\n",
    "        print(f\"Collecting data from {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "        new_data = self.get_virtual_data(start_date, end_date)\n",
    "        if not new_data.empty:\n",
    "            final_data = self.merge_and_save_data(new_data)\n",
    "            print(f\"Data saved successfully\")\n",
    "            print(f\"Total matches in database: {len(final_data)}\")\n",
    "            print(f\"Files saved as: {self.csv_filename} and {self.excel_filename}\")\n",
    "        else:\n",
    "            print(\"No new data collected\")\n",
    "\n",
    "def main(days_back=1):\n",
    "    collector = VirtualSportsCollector()\n",
    "    collector.collect_data(days_back)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()  # Default 90 days"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting data from 2024-11-20 to 2024-11-22\n",
      "Data saved successfully\n",
      "Total matches in database: 19867\n",
      "Files saved as: virtual_matches_data.csv and virtual_matches_data.xlsx\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "predizione",
   "id": "914ffe05d6d72016"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T23:00:16.637656Z",
     "start_time": "2024-11-21T22:59:39.990530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import warnings\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class SoccerPredictor:\n",
    "    def __init__(self):\n",
    "        self.team_encoder = LabelEncoder()\n",
    "        self.scaler = StandardScaler()\n",
    "        self.lstm_model = None\n",
    "        self.sequence_length = 3\n",
    "\n",
    "    def prepare_features(self, df):\n",
    "        df = df.copy()\n",
    "        df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "        df = df.sort_values('datetime')\n",
    "\n",
    "        # Base features\n",
    "        features = pd.DataFrame({\n",
    "            'datetime': df['datetime'],\n",
    "            'home_team': df['home_team'],\n",
    "            'away_team': df['away_team'],\n",
    "            'odds_1': df['odds_1'].astype(float)\n",
    "        })\n",
    "\n",
    "        # Encode teams\n",
    "        all_teams = pd.concat([df['home_team'], df['away_team']]).unique()\n",
    "        self.team_encoder.fit(all_teams)\n",
    "        features['home_team_enc'] = self.team_encoder.transform(df['home_team'])\n",
    "        features['away_team_enc'] = self.team_encoder.transform(df['away_team'])\n",
    "\n",
    "        # Calculate team stats\n",
    "        for team in all_teams:\n",
    "            home_matches = df[df['home_team'] == team]\n",
    "            away_matches = df[df['away_team'] == team]\n",
    "\n",
    "            home_rolling = home_matches['home_goals'].rolling(3, min_periods=1).mean()\n",
    "            away_rolling = away_matches['away_goals'].rolling(3, min_periods=1).mean()\n",
    "\n",
    "            team_idx = self.team_encoder.transform([team])[0]\n",
    "            features.loc[features['home_team_enc'] == team_idx, 'home_rolling_goals'] = home_rolling.values\n",
    "            features.loc[features['away_team_enc'] == team_idx, 'away_rolling_goals'] = away_rolling.values\n",
    "\n",
    "        feature_cols = ['home_team_enc', 'away_team_enc', 'odds_1', 'home_rolling_goals', 'away_rolling_goals']\n",
    "        X = features[feature_cols].fillna(0)\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "\n",
    "        y = (df['result'] == 'X').astype(int)\n",
    "        X_train, X_temp, y_train, y_temp = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "        X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "        return X_train, X_valid, X_test, y_train, y_valid, y_test, features[['datetime', 'home_team', 'away_team']]\n",
    "\n",
    "    def create_sequences(self, features):\n",
    "        sequences = [features[i:i + self.sequence_length] for i in range(len(features) - self.sequence_length + 1)]\n",
    "        return np.array(sequences)\n",
    "\n",
    "    def train_model(self, df):\n",
    "        try:\n",
    "            X_train, X_valid, X_test, y_train, y_valid, y_test, _ = self.prepare_features(df)\n",
    "\n",
    "            X_seq = self.create_sequences(X_train)\n",
    "            y_seq = y_train[self.sequence_length - 1:]\n",
    "\n",
    "            self.lstm_model = Sequential([\n",
    "                LSTM(64, input_shape=(self.sequence_length, X_train.shape[1])),\n",
    "                BatchNormalization(),\n",
    "                Dropout(0.3),\n",
    "                Dense(32, activation='relu'),\n",
    "                Dropout(0.2),\n",
    "                Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "\n",
    "            self.lstm_model.compile(\n",
    "                optimizer=Adam(0.001), loss='binary_crossentropy', metrics=['accuracy']\n",
    "            )\n",
    "\n",
    "            self.lstm_model.fit(\n",
    "                X_seq, y_seq, epochs=30, batch_size=32,\n",
    "                validation_split=0.2, callbacks=[EarlyStopping(patience=5, restore_best_weights=True)], verbose=0\n",
    "            )\n",
    "\n",
    "            lstm_pred = (self.lstm_model.predict(self.create_sequences(X_train)) > 0.5).flatten()\n",
    "\n",
    "            return {\n",
    "                'accuracy': accuracy_score(y_seq, lstm_pred),\n",
    "                'precision': precision_score(y_seq, lstm_pred),\n",
    "                'recall': recall_score(y_seq, lstm_pred),\n",
    "                'f1': f1_score(y_seq, lstm_pred)\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Training error: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def predict_draws(self, new_data):\n",
    "        try:\n",
    "            X_new, _, _, _, _, _, match_info = self.prepare_features(new_data)\n",
    "            X_seq = self.create_sequences(X_new)\n",
    "\n",
    "            lstm_probs = self.lstm_model.predict(X_seq).flatten()\n",
    "\n",
    "            min_len = min(len(lstm_probs), len(match_info['datetime'].iloc[self.sequence_length - 1:]))\n",
    "\n",
    "            predictions = pd.DataFrame({\n",
    "                'datetime': match_info['datetime'].iloc[self.sequence_length - 1: self.sequence_length - 1 + min_len],\n",
    "                'home_team': match_info['home_team'].iloc[self.sequence_length - 1: self.sequence_length - 1 + min_len],\n",
    "                'away_team': match_info['away_team'].iloc[self.sequence_length - 1: self.sequence_length - 1 + min_len],\n",
    "                'lstm_prob': lstm_probs[:min_len],\n",
    "                'draw_probability': lstm_probs[:min_len]\n",
    "            })\n",
    "\n",
    "            return predictions\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Prediction error: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        print(\"Loading data...\")\n",
    "        df = pd.read_csv('virtual_matches_data.csv')\n",
    "        print(f\"Loaded {len(df)} matches\")\n",
    "\n",
    "        print(\"\\nTraining LSTM model...\")\n",
    "        predictor = SoccerPredictor()\n",
    "        metrics = predictor.train_model(df)\n",
    "\n",
    "        print(\"\\nLSTM Model Performance:\")\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"{metric}: {value:.3f}\")\n",
    "\n",
    "        predictor.lstm_model.save('lstm_model.keras')\n",
    "        print(\"LSTM model saved successfully.\")\n",
    "\n",
    "        print(\"\\nPredicting upcoming matches...\")\n",
    "        latest_date = pd.to_datetime(df['datetime']).max()\n",
    "        future_matches = df[pd.to_datetime(df['datetime']) > latest_date - pd.Timedelta(hours=1)]\n",
    "\n",
    "        if len(future_matches) > 0:\n",
    "            predictions = predictor.predict_draws(future_matches)\n",
    "\n",
    "            print(\"\\nDraw Probabilities:\")\n",
    "            for _, row in predictions.iterrows():\n",
    "                print(f\"{row['home_team']} vs {row['away_team']}: {row['draw_probability']:.3f}\")\n",
    "        else:\n",
    "            print(\"No upcoming matches found\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "c6310d1f5de9f5d5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loaded 19818 matches\n",
      "\n",
      "Training LSTM model...\n",
      "\u001B[1m496/496\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 1ms/step\n",
      "\n",
      "LSTM Model Performance:\n",
      "accuracy: 0.995\n",
      "precision: 0.988\n",
      "recall: 0.993\n",
      "f1: 0.990\n",
      "LSTM model saved successfully.\n",
      "\n",
      "Predicting upcoming matches...\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n",
      "\n",
      "Draw Probabilities:\n",
      "ROM vs JUV: 0.000\n",
      "LAZ vs FIO: 0.000\n",
      "JUV vs MIL: 0.002\n",
      "FIO vs INT: 0.988\n",
      "ROM vs LAZ: 0.000\n",
      "NAP vs FIO: 0.000\n",
      "MIL vs LAZ: 1.000\n",
      "INT vs ROM: 0.000\n",
      "FIO vs JUV: 1.000\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Dati da prevedere.",
   "id": "5c5547cee97ff7c8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T23:00:24.683181Z",
     "start_time": "2024-11-21T23:00:16.656464Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "class VirtualOddsCollector:\n",
    "    def __init__(self):\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36',\n",
    "            'Accept': 'application/json, text/plain, */*',\n",
    "            'Accept-Encoding': 'gzip, deflate, br, zstd',\n",
    "            'Accept-Language': 'en-US,en;q=0.6',\n",
    "            'Origin': 'https://www.eurobet.it',\n",
    "            'X-EB-Accept-Language': 'it_IT',\n",
    "            'X-EB-MarketId': '5',\n",
    "            'X-EB-PlatformId': '1',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Referer': 'https://www.eurobet.it/'\n",
    "        }\n",
    "        self.data_dir = Path('data')\n",
    "        self.data_dir.mkdir(exist_ok=True)\n",
    "        self.csv_filename = self.data_dir / 'virtual_odds_detail.csv'\n",
    "        self.base_url = \"https://virtualservice.eurobet.it/virtual-detail-service/virtual-schedule/services/22/sport/{}\"\n",
    "\n",
    "        print(f\"File will be saved to: {self.csv_filename}\")\n",
    "\n",
    "    def get_match_odds(self, match_code):\n",
    "        \"\"\"Get odds for a specific match\"\"\"\n",
    "        url = self.base_url.format(match_code)\n",
    "        try:\n",
    "            print(f\"Fetching data for match code: {match_code}\")\n",
    "            response = requests.get(url, headers=self.headers, timeout=10)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            data = response.json()\n",
    "            if 'result' not in data or 'eventdetail' not in data['result']:\n",
    "                print(f\"No valid data found for match {match_code}\")\n",
    "                return None\n",
    "\n",
    "            event_info = data['result']['eventdetail']['eventInfo']\n",
    "            bet_groups = data['result']['eventdetail']['betGroupList']\n",
    "\n",
    "            match_data = {\n",
    "                'match_code': match_code,\n",
    "                'timestamp': datetime.fromtimestamp(event_info['eventData']/1000).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'home_team': event_info['teamHomeDescription'],\n",
    "                'away_team': event_info['teamAwayDescription'],\n",
    "                'channel': event_info['channelDescription'],\n",
    "                'event_code': event_info['eventCode'],\n",
    "                'program_code': event_info['programCode'],\n",
    "                'collection_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            }\n",
    "\n",
    "            for bet_group in bet_groups:\n",
    "                bet_type = bet_group['betDescription']\n",
    "                market_id = bet_group['marketId']\n",
    "\n",
    "                if bet_group['oddGroupList'] and bet_group['oddGroupList'][0]['oddList']:\n",
    "                    odds = bet_group['oddGroupList'][0]['oddList']\n",
    "\n",
    "                    for odd in odds:\n",
    "                        desc = odd['oddDescription']\n",
    "                        value = odd['oddValue']\n",
    "                        result_code = odd['resultCode']\n",
    "\n",
    "                        key_base = f\"{bet_type}_{desc}\".lower().replace('/', '_').replace(' ', '_')\n",
    "                        match_data[f\"{key_base}_odds\"] = value\n",
    "                        match_data[f\"{key_base}_code\"] = result_code\n",
    "\n",
    "            print(f\"Successfully collected odds for {match_data['home_team']} vs {match_data['away_team']}\")\n",
    "            return match_data\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Network error for match {match_code}: {e}\")\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"JSON parsing error for match {match_code}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error for match {match_code}: {e}\")\n",
    "        return None\n",
    "\n",
    "    def collect_matches(self, match_codes):\n",
    "        \"\"\"Collect odds for multiple matches\"\"\"\n",
    "        all_matches = []\n",
    "\n",
    "        print(f\"\\nStarting collection for match codes: {match_codes}\")\n",
    "\n",
    "        for match_code in match_codes:\n",
    "            match_data = self.get_match_odds(match_code)\n",
    "            if match_data:\n",
    "                all_matches.append(match_data)\n",
    "            time.sleep(1)\n",
    "\n",
    "        if not all_matches:\n",
    "            print(\"No match data collected!\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        df = pd.DataFrame(all_matches)\n",
    "\n",
    "        if self.csv_filename.exists():\n",
    "            try:\n",
    "                existing_df = pd.read_csv(self.csv_filename)\n",
    "                print(f\"Loaded {len(existing_df)} existing records\")\n",
    "                df = pd.concat([existing_df, df], ignore_index=True)\n",
    "                df = df.drop_duplicates(subset=['match_code', 'timestamp'], keep='last')\n",
    "                print(f\"After merging and removing duplicates: {len(df)} records\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading existing data: {e}\")\n",
    "\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "        df = df.sort_values('timestamp', ascending=False)\n",
    "\n",
    "        try:\n",
    "            df.to_csv(self.csv_filename, index=False)\n",
    "            print(f\"\\nSuccessfully saved data to {self.csv_filename}\")\n",
    "            print(f\"Total matches in database: {len(df)}\")\n",
    "\n",
    "            print(\"\\nPreview of saved data:\")\n",
    "            basic_cols = ['timestamp', 'home_team', 'away_team']\n",
    "            odds_cols = [col for col in df.columns if '1x2_finale' in col and 'odds' in col]\n",
    "            print(df[basic_cols + odds_cols].head())\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving data: {e}\")\n",
    "\n",
    "        return df\n",
    "\n",
    "# Variabili per generare i codici delle partite\n",
    "BASE_CODE = \"2402405106\"  # Codice base\n",
    "START_NUMBER = 1  # Numero iniziale\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Genera 5 codici incrementando di 2 ogni volta\n",
    "    match_codes = []\n",
    "    current_number = START_NUMBER\n",
    "\n",
    "    for _ in range(5):\n",
    "        match_code = f\"{BASE_CODE}_{current_number}\"\n",
    "        match_codes.append(match_code)\n",
    "        current_number += 2\n",
    "\n",
    "    collector = VirtualOddsCollector()\n",
    "    collector.collect_matches(match_codes)"
   ],
   "id": "a0dd4bab52375c25",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File will be saved to: data\\virtual_odds_detail.csv\n",
      "\n",
      "Starting collection for match codes: ['55_2402405092_416', '55_2402405092_418', '55_2402405092_420', '55_2402405092_422', '55_2402405092_424']\n",
      "Fetching data for match code: 55_2402405092_416\n",
      "Successfully collected odds for INT vs JUV\n",
      "Fetching data for match code: 55_2402405092_418\n",
      "Successfully collected odds for LAZ vs NAP\n",
      "Fetching data for match code: 55_2402405092_420\n",
      "Successfully collected odds for ROM vs FIO\n",
      "Fetching data for match code: 55_2402405092_422\n",
      "Unexpected error for match 55_2402405092_422: argument of type 'NoneType' is not iterable\n",
      "Fetching data for match code: 55_2402405092_424\n",
      "Unexpected error for match 55_2402405092_424: argument of type 'NoneType' is not iterable\n",
      "Loaded 3 existing records\n",
      "After merging and removing duplicates: 3 records\n",
      "\n",
      "Successfully saved data to data\\virtual_odds_detail.csv\n",
      "Total matches in database: 3\n",
      "\n",
      "Preview of saved data:\n",
      "            timestamp home_team away_team  1x2_finale_1_odds  \\\n",
      "5 2024-11-21 23:59:00       ROM       FIO                200   \n",
      "4 2024-11-21 23:55:00       LAZ       NAP                185   \n",
      "3 2024-11-21 23:51:00       INT       JUV                256   \n",
      "\n",
      "   1x2_finale_x_odds  1x2_finale_2_odds  \n",
      "5                334                296  \n",
      "4                351                322  \n",
      "3                320                231  \n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "prevsione",
   "id": "60662ac6667ff009"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T23:22:24.853738Z",
     "start_time": "2024-11-21T23:22:24.705947Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "class SoccerPredictor:\n",
    "    def __init__(self):\n",
    "        self.team_encoder = LabelEncoder()\n",
    "        self.scaler = StandardScaler()\n",
    "        self.lstm_model = load_model('lstm_model.keras')  # Load the pre-trained model\n",
    "\n",
    "    def prepare_features(self, df):\n",
    "        df = df.copy()\n",
    "        df['datetime'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "        # Extracting features from the CSV data\n",
    "        features = pd.DataFrame({\n",
    "            'datetime': df['timestamp'],\n",
    "            'home_team': df['home_team'],\n",
    "            'away_team': df['away_team'],\n",
    "            'odds_1': df['1x2_finale_1_odds'].astype(float),\n",
    "            'odds_x': df['1x2_finale_x_odds'].astype(float),\n",
    "            'odds_2': df['1x2_finale_2_odds'].astype(float)\n",
    "        })\n",
    "\n",
    "        # Encode teams\n",
    "        all_teams = pd.concat([df['home_team'], df['away_team']]).unique()\n",
    "        self.team_encoder.fit(all_teams)\n",
    "        features['home_team_enc'] = self.team_encoder.transform(df['home_team'])\n",
    "        features['away_team_enc'] = self.team_encoder.transform(df['away_team'])\n",
    "\n",
    "        feature_cols = ['home_team_enc', 'away_team_enc', 'odds_1', 'odds_x', 'odds_2']\n",
    "        X = features[feature_cols].fillna(0)\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "\n",
    "        return X_scaled, features[['datetime', 'home_team', 'away_team']]\n",
    "\n",
    "    def predict_draws(self, new_data):\n",
    "        try:\n",
    "            X_new, match_info = self.prepare_features(new_data)\n",
    "            lstm_probs = self.lstm_model.predict(X_new, batch_size=1).flatten()\n",
    "\n",
    "            predictions = pd.DataFrame({\n",
    "                'datetime': match_info['datetime'],\n",
    "                'home_team': match_info['home_team'],\n",
    "                'away_team': match_info['away_team'],\n",
    "                'draw_probability': lstm_probs\n",
    "            })\n",
    "\n",
    "            return predictions\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Prediction error: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        print(\"Loading data...\")\n",
    "        df = pd.read_csv('virtual_odds_detail.csv')\n",
    "        print(f\"Loaded {len(df)} matches\")\n",
    "\n",
    "        print(\"\\nPredicting new matches...\")\n",
    "        predictor = SoccerPredictor()\n",
    "\n",
    "        # Ensure 'datetime' column exists for sorting\n",
    "        if 'datetime' not in df.columns:\n",
    "            df['datetime'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "        # Filter to only predict for the two new matches\n",
    "        new_matches = df[df['datetime'] > (pd.Timestamp.now() - pd.Timedelta(hours=1))]\n",
    "        if len(new_matches) == 0:\n",
    "            print(\"No new matches found in the data.\")\n",
    "            return\n",
    "\n",
    "        predictions = predictor.predict_draws(new_matches)\n",
    "\n",
    "        if predictions.empty:\n",
    "            print(\"No predictions could be made.\")\n",
    "            return\n",
    "\n",
    "        # Display predictions\n",
    "        print(\"\\nDraw Probabilities for New Matches:\")\n",
    "        print(predictions[['home_team', 'away_team', 'draw_probability']])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "1ded759efcbcbbab",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loaded 2 matches\n",
      "\n",
      "Predicting new matches...\n",
      "Prediction error: Exception encountered when calling Sequential.call().\n",
      "\n",
      "\u001B[1mInvalid input shape for input Tensor(\"data:0\", shape=(1, 5), dtype=float32). Expected shape (None, 3, 5), but input has incompatible shape (1, 5)\u001B[0m\n",
      "\n",
      "Arguments received by Sequential.call():\n",
      "  • inputs=tf.Tensor(shape=(1, 5), dtype=float32)\n",
      "  • training=False\n",
      "  • mask=None\n",
      "Error: Exception encountered when calling Sequential.call().\n",
      "\n",
      "\u001B[1mInvalid input shape for input Tensor(\"data:0\", shape=(1, 5), dtype=float32). Expected shape (None, 3, 5), but input has incompatible shape (1, 5)\u001B[0m\n",
      "\n",
      "Arguments received by Sequential.call():\n",
      "  • inputs=tf.Tensor(shape=(1, 5), dtype=float32)\n",
      "  • training=False\n",
      "  • mask=None\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Sequential.call().\n\n\u001B[1mInvalid input shape for input Tensor(\"data:0\", shape=(1, 5), dtype=float32). Expected shape (None, 3, 5), but input has incompatible shape (1, 5)\u001B[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(1, 5), dtype=float32)\n  • training=False\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[26], line 89\u001B[0m\n\u001B[0;32m     86\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m\n\u001B[0;32m     88\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m---> 89\u001B[0m     \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[26], line 74\u001B[0m, in \u001B[0;36mmain\u001B[1;34m()\u001B[0m\n\u001B[0;32m     71\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo new matches found in the data.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m---> 74\u001B[0m predictions \u001B[38;5;241m=\u001B[39m \u001B[43mpredictor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict_draws\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnew_matches\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     76\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m predictions\u001B[38;5;241m.\u001B[39mempty:\n\u001B[0;32m     77\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo predictions could be made.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[26], line 40\u001B[0m, in \u001B[0;36mSoccerPredictor.predict_draws\u001B[1;34m(self, new_data)\u001B[0m\n\u001B[0;32m     38\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     39\u001B[0m     X_new, match_info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprepare_features(new_data)\n\u001B[1;32m---> 40\u001B[0m     lstm_probs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlstm_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_new\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mflatten()\n\u001B[0;32m     42\u001B[0m     predictions \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame({\n\u001B[0;32m     43\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdatetime\u001B[39m\u001B[38;5;124m'\u001B[39m: match_info[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdatetime\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[0;32m     44\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhome_team\u001B[39m\u001B[38;5;124m'\u001B[39m: match_info[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhome_team\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[0;32m     45\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124maway_team\u001B[39m\u001B[38;5;124m'\u001B[39m: match_info[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maway_team\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[0;32m     46\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdraw_probability\u001B[39m\u001B[38;5;124m'\u001B[39m: lstm_probs\n\u001B[0;32m     47\u001B[0m     })\n\u001B[0;32m     49\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m predictions\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    119\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m    120\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[0;32m    121\u001B[0m     \u001B[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001B[39;00m\n\u001B[1;32m--> 122\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    123\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    124\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\keras\\src\\models\\functional.py:264\u001B[0m, in \u001B[0;36mFunctional._adjust_input_rank\u001B[1;34m(self, flat_inputs)\u001B[0m\n\u001B[0;32m    262\u001B[0m             adjusted\u001B[38;5;241m.\u001B[39mappend(ops\u001B[38;5;241m.\u001B[39mexpand_dims(x, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m))\n\u001B[0;32m    263\u001B[0m             \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[1;32m--> 264\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    265\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid input shape for input \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mx\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. Expected shape \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    266\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mref_shape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, but input has incompatible shape \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mx\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    267\u001B[0m     )\n\u001B[0;32m    268\u001B[0m \u001B[38;5;66;03m# Add back metadata.\u001B[39;00m\n\u001B[0;32m    269\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(flat_inputs)):\n",
      "\u001B[1;31mValueError\u001B[0m: Exception encountered when calling Sequential.call().\n\n\u001B[1mInvalid input shape for input Tensor(\"data:0\", shape=(1, 5), dtype=float32). Expected shape (None, 3, 5), but input has incompatible shape (1, 5)\u001B[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(1, 5), dtype=float32)\n  • training=False\n  • mask=None"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "test info",
   "id": "1975fd98b0b3b8c3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T08:49:22.610194Z",
     "start_time": "2024-11-22T08:49:18.556331Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def setup_analysis():\n",
    "    \"\"\"\n",
    "    Configura l'ambiente di analisi\n",
    "    \"\"\"\n",
    "    # Impostazioni grafiche\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    sns.set_context(\"notebook\", font_scale=1.2)\n",
    "    plt.rcParams['figure.figsize'] = (12, 8)\n",
    "    plt.rcParams['savefig.dpi'] = 300\n",
    "    plt.rcParams['savefig.bbox'] = 'tight'\n",
    "\n",
    "    # Crea directory output\n",
    "    output_dir = Path('analisi_avanzata_pareggi')\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    return output_dir\n",
    "\n",
    "def prepare_advanced_data(df):\n",
    "    \"\"\"\n",
    "    Prepara i dati con feature aggiuntive per analisi avanzata\n",
    "    \"\"\"\n",
    "    # Conversioni temporali\n",
    "    df['datetime'] = pd.to_datetime(df['date'] + ' ' + df['hour'],\n",
    "                                  format='%d-%m-%Y %H:%M:%S',\n",
    "                                  dayfirst=True)\n",
    "\n",
    "    # Feature temporali\n",
    "    df['hour'] = df['datetime'].dt.hour\n",
    "    df['day_name'] = df['datetime'].dt.day_name()\n",
    "    df['day_of_week'] = df['datetime'].dt.dayofweek\n",
    "    df['month'] = df['datetime'].dt.month\n",
    "    df['week'] = df['datetime'].dt.isocalendar().week\n",
    "    df['day'] = df['datetime'].dt.date\n",
    "\n",
    "    # Sequenze e risultati\n",
    "    df['next_result'] = df['result'].shift(-1)\n",
    "    df['prev_result'] = df['result'].shift(1)\n",
    "    df['next_2_result'] = df['result'].shift(-2)\n",
    "    df['next_3_result'] = df['result'].shift(-3)\n",
    "    df['prev_2_result'] = df['result'].shift(2)\n",
    "    df['prev_3_result'] = df['result'].shift(3)\n",
    "\n",
    "    # Pattern di 3 risultati precedenti\n",
    "    df['prev_pattern'] = df['prev_3_result'].fillna('-') + df['prev_2_result'].fillna('-') + df['prev_result'].fillna('-')\n",
    "\n",
    "    # Estrai goal per analisi\n",
    "    df[['home_goals', 'away_goals']] = df['score'].str.split('-', expand=True).astype(int)\n",
    "    df['total_goals'] = df['home_goals'] + df['away_goals']\n",
    "\n",
    "    return df\n",
    "\n",
    "def analyze_sequence_patterns(df):\n",
    "    \"\"\"\n",
    "    Analizza pattern di sequenza dei pareggi\n",
    "    \"\"\"\n",
    "    # Trova sequenze di risultati\n",
    "    sequences = []\n",
    "    current_seq = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        if row['result'] == 'X':\n",
    "            current_seq.append(idx)\n",
    "        else:\n",
    "            if len(current_seq) > 0:\n",
    "                sequences.append(current_seq)\n",
    "            current_seq = []\n",
    "\n",
    "    # Analizza intervalli tra pareggi\n",
    "    intervals = []\n",
    "    for seq in sequences:\n",
    "        if len(seq) > 1:\n",
    "            for i in range(len(seq)-1):\n",
    "                intervals.append(seq[i+1] - seq[i])\n",
    "\n",
    "    return sequences, intervals\n",
    "\n",
    "def create_advanced_plots(df, output_dir):\n",
    "    \"\"\"\n",
    "    Crea grafici avanzati per l'analisi dei pattern\n",
    "    \"\"\"\n",
    "    # 11. Analisi delle sequenze di pareggi consecutive\n",
    "    sequences, intervals = analyze_sequence_patterns(df)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 12))\n",
    "\n",
    "    # Distribuzione lunghezza sequenze\n",
    "    seq_lengths = [len(seq) for seq in sequences]\n",
    "    sns.histplot(seq_lengths, ax=ax1, bins=20)\n",
    "    ax1.set_title('Distribuzione delle Lunghezze delle Sequenze di Pareggi')\n",
    "    ax1.set_xlabel('Lunghezza Sequenza')\n",
    "    ax1.set_ylabel('Frequenza')\n",
    "\n",
    "    # Distribuzione intervalli\n",
    "    sns.histplot(intervals, ax=ax2, bins=30)\n",
    "    ax2.set_title('Distribuzione degli Intervalli tra Pareggi Consecutivi')\n",
    "    ax2.set_xlabel('Numero di Partite tra Pareggi')\n",
    "    ax2.set_ylabel('Frequenza')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / '11_analisi_sequenze_avanzata.png')\n",
    "    plt.close()\n",
    "\n",
    "    # 12. Probabilità condizionata dopo diversi pattern\n",
    "    pattern_counts = df.groupby('prev_pattern')['result'].value_counts(normalize=True).unstack()\n",
    "    most_common_patterns = pattern_counts.index.value_counts().head(10).index\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 8))\n",
    "    sns.barplot(x=most_common_patterns, y=pattern_counts.loc[most_common_patterns, 'X'].fillna(0) * 100)\n",
    "    plt.xticks(rotation=45)\n",
    "    ax.set_title('Probabilità di Pareggio Dopo Pattern di 3 Risultati')\n",
    "    ax.set_xlabel('Pattern (Ultimi 3 Risultati)')\n",
    "    ax.set_ylabel('Probabilità di Pareggio (%)')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / '12_probabilita_condizionata_pattern.png')\n",
    "    plt.close()\n",
    "\n",
    "    # 13. Analisi oraria dettagliata per giorno\n",
    "    pivot_hourly = pd.crosstab([df['day_name'], df['hour']], df['result'])\n",
    "    draw_prob_hourly = pivot_hourly['X'] / pivot_hourly.sum(axis=1)\n",
    "    draw_prob_hourly = draw_prob_hourly.unstack()\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    sns.heatmap(draw_prob_hourly, annot=True, fmt='.2%', cmap='YlOrRd')\n",
    "    plt.title('Probabilità di Pareggio per Ora e Giorno')\n",
    "    plt.ylabel('Giorno della Settimana')\n",
    "    plt.xlabel('Ora del Giorno')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / '13_probabilita_oraria_giornaliera.png')\n",
    "    plt.close()\n",
    "\n",
    "    # 14. Analisi delle lunghe sequenze\n",
    "    long_sequences = [seq for seq in sequences if len(seq) >= 3]\n",
    "    if long_sequences:\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 12))\n",
    "\n",
    "        # Orari delle lunghe sequenze\n",
    "        sequence_hours = []\n",
    "        for seq in long_sequences:\n",
    "            sequence_hours.extend(df.loc[seq, 'hour'])\n",
    "\n",
    "        sns.histplot(sequence_hours, ax=ax1, bins=24)\n",
    "        ax1.set_title('Distribuzione Oraria delle Lunghe Sequenze di Pareggi')\n",
    "        ax1.set_xlabel('Ora del Giorno')\n",
    "        ax1.set_ylabel('Frequenza')\n",
    "\n",
    "        # Giorni delle lunghe sequenze\n",
    "        sequence_days = []\n",
    "        for seq in long_sequences:\n",
    "            sequence_days.extend(df.loc[seq, 'day_name'])\n",
    "\n",
    "        day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "        sns.countplot(y=sequence_days, ax=ax2, order=day_order)\n",
    "        ax2.set_title('Distribuzione Giornaliera delle Lunghe Sequenze di Pareggi')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_dir / '14_analisi_lunghe_sequenze.png')\n",
    "        plt.close()\n",
    "\n",
    "    # 15. Pattern di goal nei pareggi\n",
    "    draws = df[df['result'] == 'X']\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 12))\n",
    "\n",
    "    # Distribuzione dei goal totali\n",
    "    sns.histplot(draws['total_goals'], ax=ax1, bins=range(0, max(draws['total_goals'])+2))\n",
    "    ax1.set_title('Distribuzione dei Goal Totali nei Pareggi')\n",
    "    ax1.set_xlabel('Numero Totale di Goal')\n",
    "    ax1.set_ylabel('Frequenza')\n",
    "\n",
    "    # Heatmap home vs away goals\n",
    "    goal_matrix = pd.crosstab(draws['home_goals'], draws['away_goals'])\n",
    "    sns.heatmap(goal_matrix, annot=True, fmt='d', cmap='YlOrRd', ax=ax2)\n",
    "    ax2.set_title('Distribuzione dei Goal Casa vs Trasferta nei Pareggi')\n",
    "    ax2.set_xlabel('Goal Trasferta')\n",
    "    ax2.set_ylabel('Goal Casa')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / '15_pattern_goal_pareggi.png')\n",
    "    plt.close()\n",
    "\n",
    "    return True\n",
    "\n",
    "def create_additional_advanced_plots(df, output_dir):\n",
    "    \"\"\"\n",
    "    Crea ulteriori grafici avanzati per l'analisi\n",
    "    \"\"\"\n",
    "    # Continua con i grafici 16-20...\n",
    "    # [Questa funzione continuerà nella prossima parte del codice]\n",
    "    pass\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Setup iniziale\n",
    "        output_dir = setup_analysis()\n",
    "        print(f\"Directory output creata: {output_dir}\")\n",
    "\n",
    "        # Carica e prepara i dati\n",
    "        df = pd.read_csv('virtual_matches_data.csv')\n",
    "        df = prepare_advanced_data(df)\n",
    "        print(\"Dati preparati con successo\")\n",
    "\n",
    "        # Crea i primi grafici avanzati\n",
    "        create_advanced_plots(df, output_dir)\n",
    "        print(\"Prima serie di grafici avanzati creata con successo\")\n",
    "\n",
    "        # Crea ulteriori grafici avanzati\n",
    "        create_additional_advanced_plots(df, output_dir)\n",
    "        print(\"Seconda serie di grafici avanzati creata con successo\")\n",
    "\n",
    "        print(\"\\nAnalisi completata! Tutti i grafici sono stati salvati.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Errore durante l'esecuzione: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "b57ba18eb8e8d1f1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory output creata: analisi_avanzata_pareggi\n",
      "Dati preparati con successo\n",
      "Prima serie di grafici avanzati creata con successo\n",
      "Seconda serie di grafici avanzati creata con successo\n",
      "\n",
      "Analisi completata! Tutti i grafici sono stati salvati.\n"
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
